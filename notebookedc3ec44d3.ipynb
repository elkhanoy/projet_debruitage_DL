{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b92a0f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:49:58.249029Z",
     "iopub.status.busy": "2023-01-12T15:49:58.248578Z",
     "iopub.status.idle": "2023-01-12T15:50:09.123576Z",
     "shell.execute_reply": "2023-01-12T15:50:09.122443Z"
    },
    "papermill": {
     "duration": 10.885422,
     "end_time": "2023-01-12T15:50:09.126515",
     "exception": false,
     "start_time": "2023-01-12T15:49:58.241093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d8652d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:09.136443Z",
     "iopub.status.busy": "2023-01-12T15:50:09.135790Z",
     "iopub.status.idle": "2023-01-12T15:50:13.345141Z",
     "shell.execute_reply": "2023-01-12T15:50:13.343794Z"
    },
    "id": "4p3DVILGgLL8",
    "papermill": {
     "duration": 4.219494,
     "end_time": "2023-01-12T15:50:13.350450",
     "exception": false,
     "start_time": "2023-01-12T15:50:09.130956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable \n",
    "import h5py\n",
    "from torch.utils.data import DataLoader, Dataset, BatchSampler, Sampler\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pylab as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e46304c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:13.367988Z",
     "iopub.status.busy": "2023-01-12T15:50:13.367369Z",
     "iopub.status.idle": "2023-01-12T15:50:13.386829Z",
     "shell.execute_reply": "2023-01-12T15:50:13.385759Z"
    },
    "papermill": {
     "duration": 0.03323,
     "end_time": "2023-01-12T15:50:13.391510",
     "exception": false,
     "start_time": "2023-01-12T15:50:13.358280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__results__.html',\n",
       " 'dataset_babble_v2.hdf5',\n",
       " '__notebook__.ipynb',\n",
       " '__output__.json',\n",
       " 'custom.css']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/kaggle/input/download-dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29db80e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:13.405535Z",
     "iopub.status.busy": "2023-01-12T15:50:13.405092Z",
     "iopub.status.idle": "2023-01-12T15:50:13.480420Z",
     "shell.execute_reply": "2023-01-12T15:50:13.479581Z"
    },
    "id": "xfOo5AKGjyHH",
    "papermill": {
     "duration": 0.084746,
     "end_time": "2023-01-12T15:50:13.482949",
     "exception": false,
     "start_time": "2023-01-12T15:50:13.398203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01da01af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:13.495382Z",
     "iopub.status.busy": "2023-01-12T15:50:13.495072Z",
     "iopub.status.idle": "2023-01-12T15:50:13.500684Z",
     "shell.execute_reply": "2023-01-12T15:50:13.499829Z"
    },
    "papermill": {
     "duration": 0.017788,
     "end_time": "2023-01-12T15:50:13.506226",
     "exception": false,
     "start_time": "2023-01-12T15:50:13.488438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de82d22e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:13.520162Z",
     "iopub.status.busy": "2023-01-12T15:50:13.519693Z",
     "iopub.status.idle": "2023-01-12T15:50:13.546675Z",
     "shell.execute_reply": "2023-01-12T15:50:13.545843Z"
    },
    "id": "OR424bCeoZRW",
    "papermill": {
     "duration": 0.03744,
     "end_time": "2023-01-12T15:50:13.549297",
     "exception": false,
     "start_time": "2023-01-12T15:50:13.511857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion=None, epoch=None, log=None):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for data, target in loader:\n",
    "        # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        # data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "        # target = scaler.transform(target.reshape(-1, target.shape[-1])).reshape(target.shape)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = torch.squeeze(model(data))\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss += criterion(output, target.to(torch.float32)).item()\n",
    "\n",
    "    if criterion is not None:\n",
    "        loss /= len(loader.dataset)\n",
    "\n",
    "    print('Average loss: {:.4f}\\n'.format(loss))\n",
    "\n",
    "\n",
    "class RandomBatchSampler(Sampler):\n",
    "    \"\"\"Sampling class to create random sequential batches from a given dataset\n",
    "    E.g. if data is [1,2,3,4] with bs=2. Then first batch, [[1,2], [3,4]] then shuffle batches -> [[3,4],[1,2]]\n",
    "    This is useful for cases when you are interested in 'weak shuffling'\n",
    "    :param dataset: dataset you want to batch\n",
    "    :type dataset: torch.utils.data.Dataset\n",
    "    :param batch_size: batch size\n",
    "    :type batch_size: int\n",
    "    :returns: generator object of shuffled batch indices\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_length = len(dataset)\n",
    "        self.n_batches = self.dataset_length / self.batch_size\n",
    "        self.batch_ids = torch.randperm(int(self.n_batches))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for id in self.batch_ids:\n",
    "            idx = torch.arange(id * self.batch_size, (id + 1) * self.batch_size)\n",
    "            for index in idx:\n",
    "                yield int(index)\n",
    "        if int(self.n_batches) < self.n_batches:\n",
    "            idx = torch.arange(int(self.n_batches) * self.batch_size, self.dataset_length)\n",
    "            for index in idx:\n",
    "                yield int(index)\n",
    "\n",
    "\n",
    "def fast_loader(dataset, batch_size=32, drop_last=False, transforms=None):\n",
    "    \"\"\"Implements fast loading by taking advantage of .h5 dataset\n",
    "    The .h5 dataset has a speed bottleneck that scales (roughly) linearly with the number\n",
    "    of calls made to it. This is because when queries are made to it, a search is made to find\n",
    "    the data item at that index. However, once the start index has been found, taking the next items\n",
    "    does not require any more significant computation. So indexing data[start_index: start_index+batch_size]\n",
    "    is almost the same as just data[start_index]. The fast loading scheme takes advantage of this. However,\n",
    "    because the goal is NOT to load the entirety of the data in memory at once, weak shuffling is used instead of\n",
    "    strong shuffling.\n",
    "    :param dataset: a dataset that loads data from .h5 files\n",
    "    :type dataset: torch.utils.data.Dataset\n",
    "    :param batch_size: size of data to batch\n",
    "    :type batch_size: int\n",
    "    :param drop_last: flag to indicate if last batch will be dropped (if size < batch_size)\n",
    "    :type drop_last: bool\n",
    "    :returns: dataloading that queries from data using shuffled batches\n",
    "    :rtype: torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=None,  # must be disabled when using samplers\n",
    "        sampler=BatchSampler(RandomBatchSampler(dataset, batch_size), batch_size=batch_size, drop_last=drop_last), \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, file_path, operation = 'train'):\n",
    "        self.file_path = file_path\n",
    "        self.x_dataset_name = 'x_' + operation\n",
    "        self.y_dataset_name = 'y_' + operation\n",
    "        self.length = None\n",
    "\n",
    "        with h5py.File(self.file_path, 'r') as hf:\n",
    "            \n",
    "            self.length = len(hf.get(self.x_dataset_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _open_hdf5(self):\n",
    "        self._hf = h5py.File(self.file_path, 'r')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not hasattr(self, '_hf'):\n",
    "            self._open_hdf5()\n",
    "\n",
    "        x = self._hf[self.x_dataset_name][index]\n",
    "        y = self._hf[self.y_dataset_name][index]\n",
    "\n",
    "        x = (torch.from_numpy(x)).to(torch.float32)\n",
    "        y = (torch.from_numpy(y)).to(torch.float32)\n",
    "        return (x, y)\n",
    "\n",
    "def get_train_loader_hdf5(batch_size):\n",
    "    print('Train: ', end=\"\")\n",
    "    train_dataset = HDF5Dataset('/kaggle/input/download-dataset/dataset_babble_v2.hdf5', operation = 'train')\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "    #                           shuffle=True, num_workers=num_workers, drop_last= False)\n",
    "    train_loader = fast_loader(train_dataset, batch_size=batch_size, drop_last=False)\n",
    "    print('Found', len(train_dataset), ' train samples')\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def get_test_loader_hdf5(batch_size):\n",
    "    print('Test: ', end=\"\")\n",
    "    test_dataset = HDF5Dataset('/kaggle/input/download-dataset/dataset_babble_v2.hdf5', operation = 'test')\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "    #                           shuffle=True, num_workers=num_workers, drop_last= False)\n",
    "    test_loader = fast_loader(test_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "    print('Found', len(test_dataset), ' test samples')\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3f9b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:13.562098Z",
     "iopub.status.busy": "2023-01-12T15:50:13.561765Z",
     "iopub.status.idle": "2023-01-12T15:50:17.357903Z",
     "shell.execute_reply": "2023-01-12T15:50:17.356942Z"
    },
    "id": "gcfQ_gjto1p9",
    "papermill": {
     "duration": 3.805099,
     "end_time": "2023-01-12T15:50:17.360382",
     "exception": false,
     "start_time": "2023-01-12T15:50:13.555283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_as = 'kaggle_CNN_V2'\n",
    "max_epochs = 60\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "model_file = 'models/' + save_as + '.pt'\n",
    "\n",
    "layout = {\n",
    "    \"ABCDE\": {\n",
    "        \"loss\": [\"Multiline\", [\"loss/train\", \"loss/validation\"]]    \n",
    "        },\n",
    "}\n",
    "\n",
    "writer = SummaryWriter('experiments/' + save_as)\n",
    "writer.add_custom_scalars(layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b8c480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:17.370337Z",
     "iopub.status.busy": "2023-01-12T15:50:17.369645Z",
     "iopub.status.idle": "2023-01-12T15:50:17.400555Z",
     "shell.execute_reply": "2023-01-12T15:50:17.399733Z"
    },
    "id": "-GXKzkO_pcYf",
    "papermill": {
     "duration": 0.038118,
     "end_time": "2023-01-12T15:50:17.402577",
     "exception": false,
     "start_time": "2023-01-12T15:50:17.364459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv9 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv11 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv13 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv14 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv15 = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x.to(torch.float32)).view(-1, 1, 25, 257)\n",
    "    \n",
    "        x1 = self.pool1(self.relu(self.conv2(self.relu(self.conv1(x)))))\n",
    "        x2 = self.pool2(self.relu(self.conv4(self.relu(self.conv3(x1)))))\n",
    "        x3 = self.pool3(self.relu(self.conv6(self.relu(self.conv5(x2)))))\n",
    "        x4 = self.relu(self.conv8(self.relu(self.conv7(x3))))\n",
    "\n",
    "        x5 = self.up1(x4)\n",
    "        print(x3.shape)\n",
    "        print(\"sssss \\n\")\n",
    "        print(x5.shape)\n",
    "        x5 = torch.cat((x5, x3), dim=1)\n",
    "        x5 = self.relu(self.conv10(self.relu(self.conv9(x5))))\n",
    "\n",
    "        x6 = self.up2(x5)\n",
    "        x6 = torch.cat((x6, x2), dim=1)\n",
    "        x6 = self.relu(self.conv12(self.relu(self.conv11(x6))))\n",
    "\n",
    "        x7 = self.up3(x6)\n",
    "        x7 = torch.cat((x7, x1), dim=1)\n",
    "        x7 = self.relu(self.conv14(self.relu(self.conv13(x7))))\n",
    "\n",
    "        x8 = self.conv15(x7)\n",
    "        \n",
    "        out = x8.view(-1, 125, 257)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.fc_1 =  nn.Linear(257, 257) #fully connected 1\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=257, hidden_size=257,\n",
    "                          num_layers=4, batch_first=True) #lstm\n",
    "\n",
    "        self.fc_2 =  nn.Linear(257, 257) #fully connected 2\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = (x.to(torch.float32)).view(-1, 25, 257)\n",
    "        out = self.relu(self.fc_1(out))\n",
    "\n",
    "        h_0 = Variable(torch.zeros(4, out.size(0), 257)).to(device) #hidden state\n",
    "        c_0 = Variable(torch.zeros(4, out.size(0), 257)).to(device) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(out, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        out = self.fc_2(output)\n",
    "        out = self.relu(out)\n",
    "        out = out.view(-1, 125, 257) #reshaping the data for Dense layer next\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.c1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 7) #CNN\n",
    "        self.b1 = nn.BatchNorm2d(num_features = 8)\n",
    "        self.c2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 5) #CNN\n",
    "        self.b2 = nn.BatchNorm2d(num_features = 16)\n",
    "        self.c3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3) #CNN\n",
    "        self.b3 = nn.BatchNorm2d(num_features = 32)\n",
    "        self.t1 = nn.ConvTranspose2d(in_channels= 32, out_channels= 16, kernel_size=3)\n",
    "        self.b4 = nn.BatchNorm2d(num_features = 16)\n",
    "        self.t2 = nn.ConvTranspose2d(in_channels= 16, out_channels= 8, kernel_size=5)\n",
    "        self.b5 = nn.BatchNorm2d(num_features = 8)\n",
    "        self.t3 = nn.ConvTranspose2d(in_channels= 8, out_channels= 1, kernel_size=7)\n",
    "        self.b6 = nn.BatchNorm2d(num_features = 1)\n",
    "\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        x = (x.to(torch.float32)).view(-1, 1, 25, 257)\n",
    "        out = self.c1(x) #lstm with input, hidden, and internal state\n",
    "        out = self.relu(self.b1(out))\n",
    "        out = self.c2(out)\n",
    "        out = self.relu(self.b2(out))\n",
    "        out = self.c3(out)\n",
    "        out = self.relu(self.b3(out))\n",
    "        out = self.t1(out)\n",
    "        out = self.relu(self.b4(out))\n",
    "        out = self.t2(out)\n",
    "        out = self.relu(self.b5(out))\n",
    "        out = self.t3(out)\n",
    "        out = self.relu(self.b6(out))\n",
    "        out = out.view(-1, 125, 257)\n",
    "\n",
    "        return out\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(FCNN, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(257, 512) #fully connected 1\n",
    "        self.fc_2 = nn.Linear((512), 512) #fully connected 2\n",
    "        self.fc_3 = nn.Linear(512, 512) #fully connected 2\n",
    "        self.fc_4 = nn.Linear(512, 512) #fully connected 2\n",
    "        self.fc_5 = nn.Linear(512, 257) #fully connected last layer\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = x.to(torch.float32)\n",
    "\n",
    "        out = self.fc_1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc_2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc_3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc_4(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc_5(out)\n",
    "        # out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4cf7db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:17.411872Z",
     "iopub.status.busy": "2023-01-12T15:50:17.411562Z",
     "iopub.status.idle": "2023-01-12T15:50:17.426477Z",
     "shell.execute_reply": "2023-01-12T15:50:17.425670Z"
    },
    "id": "73aVxWQ7p8sI",
    "papermill": {
     "duration": 0.022178,
     "end_time": "2023-01-12T15:50:17.428573",
     "exception": false,
     "start_time": "2023-01-12T15:50:17.406395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_main(use_model):\n",
    "\n",
    "    model_file = save_as + '.pt'\n",
    "\n",
    "    if (use_model == 'lstm'):\n",
    "        model = LSTM().to(device)\n",
    "#         summary(model, (2, 125, 257))\n",
    "\n",
    "    elif(use_model == 'fcn'):\n",
    "        model = FCNN().to(device)\n",
    "        summary(model, (2, 125, 257))\n",
    "\n",
    "    elif(use_model == 'cnn'):\n",
    "        model = CNN().to(device)\n",
    "        summary(model, (2,1, 125, 257))\n",
    "\n",
    "    else:\n",
    "        model = Autoencoder().to(device)\n",
    "#         summary(model, (2,1, 125, 257))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(model)\n",
    "    print(f'\\n ========== Nb parametres du modele : {count_parameters(model)} ========== \\n')\n",
    "    # summary(model, (2, 125, 257))\n",
    "\n",
    "    train_loader = get_train_loader_hdf5(batch_size)\n",
    "    validation_loader = get_test_loader_hdf5(batch_size)\n",
    "    best_val_loss = None\n",
    "    best_val_epoch = 0\n",
    "    max_stagnation = 5\n",
    "    early_stop = False\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        start_time = datetime.now()\n",
    "        # Set model to training mode\n",
    "        model.train(True)\n",
    "        epoch_loss = 0.\n",
    "        # Loop over each batch from the training set\n",
    "        for data, target in train_loader:\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Zero gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Pass data through the network\n",
    "            output = model(data)\n",
    "            output = torch.squeeze(output)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        writer.add_scalar(\"loss/train\", epoch_loss, epoch)\n",
    "\n",
    "        print('Train Epoch: {}, Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "\n",
    "        # train(model, train_loader, criterion, optimizer, epoch, log)\n",
    "\n",
    "        ## VALIDATION\n",
    "        with torch.no_grad():\n",
    "            print('\\nValidation:')\n",
    "            # evaluate(model, validation_loader, criterion, epoch, log)\n",
    "            model.eval()\n",
    "            loss = 0.\n",
    "            snr = 0.\n",
    "            for data, target in validation_loader:\n",
    "                # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                # data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "                # target = scaler.transform(target.reshape(-1, target.shape[-1])).reshape(target.shape)\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = torch.squeeze(model(data))\n",
    "                # snr += mix.compute_SNR(target, output)\n",
    "                loss += criterion(output, target).item()\n",
    "            \n",
    "            loss /= len(validation_loader.dataset)\n",
    "            # snr /= len(validation_loader.dataset)\n",
    "            writer.add_scalar(\"loss/validation\", loss, epoch)\n",
    "            # writer.add_scalar(\"loss/val_SNR\", snr, epoch)\n",
    "            print('Average loss: {:.4f}\\n'.format(loss))\n",
    "            if ((best_val_loss is None) or (best_val_loss > loss)):\n",
    "                best_val_loss, best_val_epoch = loss, epoch\n",
    "            if (best_val_epoch < (epoch - max_stagnation)):\n",
    "                # nothing is improving for a while\n",
    "                early_stop = True\n",
    "\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        epoch_time = (end_time - start_time).total_seconds()\n",
    "        txt = 'Epoch took {:.2f} seconds.'.format(epoch_time)\n",
    "        print(txt)\n",
    "        if(early_stop):\n",
    "            print(f\"Stagnation reached\")\n",
    "            break\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45da4f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:50:17.437233Z",
     "iopub.status.busy": "2023-01-12T15:50:17.436972Z",
     "iopub.status.idle": "2023-01-12T16:56:59.938969Z",
     "shell.execute_reply": "2023-01-12T16:56:59.937781Z"
    },
    "executionInfo": {
     "elapsed": 3560922,
     "status": "ok",
     "timestamp": 1671491247890,
     "user": {
      "displayName": "Yassine El Khanoussi",
      "userId": "16456168710578929959"
     },
     "user_tz": -60
    },
    "id": "SOvkft0Bqior",
    "outputId": "250d1da0-0139-4b18-e55e-d2eccde42c7d",
    "papermill": {
     "duration": 4002.509062,
     "end_time": "2023-01-12T16:56:59.941346",
     "exception": false,
     "start_time": "2023-01-12T15:50:17.432284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 8, 19, 251]             400\n",
      "       BatchNorm2d-2           [-1, 8, 19, 251]              16\n",
      "              ReLU-3           [-1, 8, 19, 251]               0\n",
      "            Conv2d-4          [-1, 16, 15, 247]           3,216\n",
      "       BatchNorm2d-5          [-1, 16, 15, 247]              32\n",
      "              ReLU-6          [-1, 16, 15, 247]               0\n",
      "            Conv2d-7          [-1, 32, 13, 245]           4,640\n",
      "       BatchNorm2d-8          [-1, 32, 13, 245]              64\n",
      "              ReLU-9          [-1, 32, 13, 245]               0\n",
      "  ConvTranspose2d-10          [-1, 16, 15, 247]           4,624\n",
      "      BatchNorm2d-11          [-1, 16, 15, 247]              32\n",
      "             ReLU-12          [-1, 16, 15, 247]               0\n",
      "  ConvTranspose2d-13           [-1, 8, 19, 251]           3,208\n",
      "      BatchNorm2d-14           [-1, 8, 19, 251]              16\n",
      "             ReLU-15           [-1, 8, 19, 251]               0\n",
      "  ConvTranspose2d-16           [-1, 1, 25, 257]             393\n",
      "      BatchNorm2d-17           [-1, 1, 25, 257]               2\n",
      "             ReLU-18           [-1, 1, 25, 257]               0\n",
      "================================================================\n",
      "Total params: 16,643\n",
      "Trainable params: 16,643\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 6.94\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 7.25\n",
      "----------------------------------------------------------------\n",
      "CNN(\n",
      "  (c1): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (b1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c2): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (t1): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (b4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (t2): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (b5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (t3): ConvTranspose2d(8, 1, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (b6): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " ========== Nb parametres du modele : 16643 ========== \n",
      "\n",
      "Train: Found 16158  train samples\n",
      "Test: Found 1877  test samples\n",
      "Train Epoch: 1, Loss: 0.0002\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 90.19 seconds.\n",
      "Train Epoch: 2, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 75.26 seconds.\n",
      "Train Epoch: 3, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 74.62 seconds.\n",
      "Train Epoch: 4, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 75.20 seconds.\n",
      "Train Epoch: 5, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 75.85 seconds.\n",
      "Train Epoch: 6, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 75.42 seconds.\n",
      "Train Epoch: 7, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 75.16 seconds.\n",
      "Train Epoch: 8, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 75.54 seconds.\n",
      "Train Epoch: 9, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 76.33 seconds.\n",
      "Train Epoch: 10, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 76.21 seconds.\n",
      "Train Epoch: 11, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 76.44 seconds.\n",
      "Train Epoch: 12, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.18 seconds.\n",
      "Train Epoch: 13, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.11 seconds.\n",
      "Train Epoch: 14, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.20 seconds.\n",
      "Train Epoch: 15, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 80.91 seconds.\n",
      "Train Epoch: 16, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 80.08 seconds.\n",
      "Train Epoch: 17, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 79.79 seconds.\n",
      "Train Epoch: 18, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.92 seconds.\n",
      "Train Epoch: 19, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 81.24 seconds.\n",
      "Train Epoch: 20, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.33 seconds.\n",
      "Train Epoch: 21, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 79.65 seconds.\n",
      "Train Epoch: 22, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.96 seconds.\n",
      "Train Epoch: 23, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.12 seconds.\n",
      "Train Epoch: 24, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.13 seconds.\n",
      "Train Epoch: 25, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 76.93 seconds.\n",
      "Train Epoch: 26, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.64 seconds.\n",
      "Train Epoch: 27, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 76.29 seconds.\n",
      "Train Epoch: 28, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 80.06 seconds.\n",
      "Train Epoch: 29, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.81 seconds.\n",
      "Train Epoch: 30, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 80.90 seconds.\n",
      "Train Epoch: 31, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.71 seconds.\n",
      "Train Epoch: 32, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 79.11 seconds.\n",
      "Train Epoch: 33, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 79.19 seconds.\n",
      "Train Epoch: 34, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.77 seconds.\n",
      "Train Epoch: 35, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.61 seconds.\n",
      "Train Epoch: 36, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.44 seconds.\n",
      "Train Epoch: 37, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.45 seconds.\n",
      "Train Epoch: 38, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 76.58 seconds.\n",
      "Train Epoch: 39, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 79.93 seconds.\n",
      "Train Epoch: 40, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 79.23 seconds.\n",
      "Train Epoch: 41, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.31 seconds.\n",
      "Train Epoch: 42, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.87 seconds.\n",
      "Train Epoch: 43, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 80.60 seconds.\n",
      "Train Epoch: 44, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 80.33 seconds.\n",
      "Train Epoch: 45, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 80.49 seconds.\n",
      "Train Epoch: 46, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 79.43 seconds.\n",
      "Train Epoch: 47, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 78.60 seconds.\n",
      "Train Epoch: 48, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.36 seconds.\n",
      "Train Epoch: 49, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 76.06 seconds.\n",
      "Train Epoch: 50, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.06 seconds.\n",
      "Train Epoch: 51, Loss: 0.0001\n",
      "\n",
      "Validation:\n",
      "Average loss: 0.0001\n",
      "\n",
      "Epoch took 77.65 seconds.\n",
      "Stagnation reached\n"
     ]
    }
   ],
   "source": [
    "use_model = 'cnn'\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "train_main(use_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31d7a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 10095,
     "status": "ok",
     "timestamp": 1671484461780,
     "user": {
      "displayName": "Yassine El Khanoussi",
      "userId": "16456168710578929959"
     },
     "user_tz": -60
    },
    "id": "43zdUDxz-j6Z",
    "outputId": "bd9f5ecb-80e6-4c1a-d24e-39abc2ade31c",
    "papermill": {
     "duration": 0.0106,
     "end_time": "2023-01-12T16:56:59.961782",
     "exception": false,
     "start_time": "2023-01-12T16:56:59.951182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a0a75",
   "metadata": {
    "executionInfo": {
     "elapsed": 148603,
     "status": "ok",
     "timestamp": 1671484133249,
     "user": {
      "displayName": "Yassine El Khanoussi",
      "userId": "16456168710578929959"
     },
     "user_tz": -60
    },
    "id": "9cmbqlE1HwUf",
    "outputId": "4a6507a4-c905-4e4b-8735-0a99f825d825",
    "papermill": {
     "duration": 0.009203,
     "end_time": "2023-01-12T16:56:59.980330",
     "exception": false,
     "start_time": "2023-01-12T16:56:59.971127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4033.228266,
   "end_time": "2023-01-12T16:57:03.588057",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-12T15:49:50.359791",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
